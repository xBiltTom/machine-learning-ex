import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import os
import numpy as np

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from ejercicios.ejercicio3.procesamiento import procesar_iris_completo


def mostrar_ejercicio3():
    """
    Vista del Ejercicio 3: Dataset Iris
    Objetivo: Implementar un flujo completo de preprocesamiento y visualizar resultados.
    """
    st.header("üå∏ Ejercicio 3: Dataset Iris")
    
    st.markdown("""
    ### üìã Objetivo
    Implementar un **flujo completo de preprocesamiento** y visualizar resultados del dataset Iris.
    
    ### üîß Instrucciones implementadas:
    1. ‚úÖ Carga del dataset desde sklearn.datasets
    2. ‚úÖ Conversi√≥n a DataFrame con nombres de columnas
    3. ‚úÖ Estandarizaci√≥n con StandardScaler()
    4. ‚úÖ Divisi√≥n del dataset (70% entrenamiento, 30% prueba)
    5. ‚úÖ Gr√°fico de dispersi√≥n (sepal length vs petal length) por clase
    
    ### üìà Salidas esperadas:
    - Gr√°fico de dispersi√≥n con colores por clase
    - Estad√≠sticas descriptivas del dataset estandarizado
    """)
    
    st.markdown("---")
    
    if st.button("üîÑ Ejecutar Procesamiento Completo", type="primary", use_container_width=True):
        with st.spinner("Procesando dataset Iris..."):
            try:
                processor, resumen = procesar_iris_completo()
                
                st.session_state['iris_processor'] = processor
                st.session_state['iris_resumen'] = resumen
                st.success("‚úÖ Procesamiento completado exitosamente!")
                
            except Exception as e:
                st.error(f"‚ùå Error durante el procesamiento: {str(e)}")
                return
    
    if 'iris_resumen' in st.session_state:
        resumen = st.session_state['iris_resumen']
        processor = st.session_state['iris_processor']
        
        # 1. CARGA DEL DATASET
        st.markdown("## üì• 1. Carga del Dataset desde scikit-learn")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total de Muestras", resumen['carga']['n_samples'])
        with col2:
            st.metric("Caracter√≠sticas", resumen['carga']['n_features'])
        with col3:
            st.metric("Clases", len(resumen['carga']['target_names']))
        with col4:
            st.metric("Dataset", "Iris")
        
        with st.expander("‚ÑπÔ∏è Informaci√≥n del Dataset"):
            st.markdown("**Caracter√≠sticas del dataset:**")
            for i, feature in enumerate(resumen['carga']['feature_names'], 1):
                st.write(f"{i}. {feature}")
            
            st.markdown("**Clases (especies):**")
            for i, target in enumerate(resumen['carga']['target_names'], 1):
                st.write(f"{i}. {target}")
        
        # 2. CONVERSI√ìN A DATAFRAME
        st.markdown("---")
        st.markdown("## üìä 2. Conversi√≥n a DataFrame")
        
        st.markdown(f"Dataset convertido a DataFrame con **{resumen['dataframe']['shape'][0]} filas** y **{resumen['dataframe']['shape'][1]} columnas**")
        
        with st.expander("üìã Ver Dataset Original (primeros 10 registros)"):
            st.dataframe(resumen['df_original'].head(10), use_container_width=True)
        
        # Distribuci√≥n de clases
        st.markdown("### üìä Distribuci√≥n de Clases")
        
        cols = st.columns(len(resumen['exploracion']['info_clases']))
        for i, (especie, info) in enumerate(resumen['exploracion']['info_clases'].items()):
            with cols[i]:
                st.metric(especie.capitalize(), info['cantidad'], info['porcentaje'])
        
        # 3. ESTANDARIZACI√ìN
        st.markdown("---")
        st.markdown("## ‚öñÔ∏è 3. Estandarizaci√≥n con StandardScaler")
        
        st.markdown("""
        Se aplic√≥ **StandardScaler** para estandarizar las caracter√≠sticas.
        La estandarizaci√≥n transforma los datos para que tengan **media = 0** y **desviaci√≥n est√°ndar = 1**.
        """)
        
        # Comparaci√≥n antes y despu√©s
        col1, col2 = st.columns(2)
        
        stats_antes = []
        stats_despues = []
        
        for col in resumen['estandarizacion']['columnas_estandarizadas']:
            antes = resumen['estandarizacion']['estadisticas_antes'][col]
            despues = resumen['estandarizacion']['estadisticas_despues'][col]
            
            # Nombre simplificado
            nombre_corto = col.replace(' (cm)', '').title()
            
            stats_antes.append({
                'Caracter√≠stica': nombre_corto,
                'Media': f"{antes['media']:.2f}",
                'Desv. Est.': f"{antes['std']:.2f}",
                'Min': f"{antes['min']:.2f}",
                'Max': f"{antes['max']:.2f}"
            })
            
            stats_despues.append({
                'Caracter√≠stica': nombre_corto,
                'Media': f"{despues['media']:.4f}",
                'Desv. Est.': f"{despues['std']:.4f}",
                'Min': f"{despues['min']:.2f}",
                'Max': f"{despues['max']:.2f}"
            })
        
        with col1:
            st.markdown("**üìä Antes de la Estandarizaci√≥n**")
            st.dataframe(pd.DataFrame(stats_antes), use_container_width=True)
        
        with col2:
            st.markdown("**ÔøΩ Despu√©s de la Estandarizaci√≥n**")
            st.dataframe(pd.DataFrame(stats_despues), use_container_width=True)
        
        # 4. DIVISI√ìN DE DATOS
        st.markdown("---")
        st.markdown("## ‚úÇÔ∏è 4. Divisi√≥n del Dataset")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total de Registros", resumen['division']['total_registros'])
        
        with col2:
            st.metric(
                "Conjunto de Entrenamiento", 
                f"{resumen['division']['train_shape']['X_train'][0]} registros",
                f"{resumen['division']['porcentajes']['train']}"
            )
        
        with col3:
            st.metric(
                "Conjunto de Prueba", 
                f"{resumen['division']['test_shape']['X_test'][0]} registros",
                f"{resumen['division']['porcentajes']['test']}"
            )
        
        # Dimensiones
        st.markdown("### üìê Dimensiones de los Conjuntos")
        
        dimensiones_data = {
            'Conjunto': ['X_train', 'y_train', 'X_test', 'y_test'],
            'Dimensiones': [
                str(resumen['division']['train_shape']['X_train']),
                str(resumen['division']['train_shape']['y_train']),
                str(resumen['division']['test_shape']['X_test']),
                str(resumen['division']['test_shape']['y_test'])
            ],
            'Descripci√≥n': [
                f"{resumen['division']['num_caracteristicas']} caracter√≠sticas",
                "Variable objetivo (target)",
                f"{resumen['division']['num_caracteristicas']} caracter√≠sticas",
                "Variable objetivo (target)"
            ]
        }
        
        st.dataframe(pd.DataFrame(dimensiones_data), use_container_width=True)
        
        # Distribuci√≥n por conjunto
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Distribuci√≥n en Entrenamiento**")
            dist_train = resumen['division']['distribucion_clases_train']
            dist_train_df = pd.DataFrame({
                'Clase': [resumen['target_names'][k] for k in sorted(dist_train.keys())],
                'Cantidad': [dist_train[k] for k in sorted(dist_train.keys())]
            })
            st.dataframe(dist_train_df, use_container_width=True)
        
        with col2:
            st.markdown("**Distribuci√≥n en Prueba**")
            dist_test = resumen['division']['distribucion_clases_test']
            dist_test_df = pd.DataFrame({
                'Clase': [resumen['target_names'][k] for k in sorted(dist_test.keys())],
                'Cantidad': [dist_test[k] for k in sorted(dist_test.keys())]
            })
            st.dataframe(dist_test_df, use_container_width=True)
        
        # 5. VISUALIZACI√ìN - GR√ÅFICO DE DISPERSI√ìN
        st.markdown("---")
        st.markdown("## üìà 5. Visualizaci√≥n: Sepal Length vs Petal Length por Clase")
        
        # Colores para cada clase
        colores = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        nombres_especies = resumen['target_names']
        
        # Crear gr√°ficos lado a lado
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**üìä Datos Originales**")
            fig1, ax1 = plt.subplots(figsize=(8, 6))
            
            viz_original = resumen['visualizacion']['original']
            
            for i, especie in enumerate(nombres_especies):
                mask = [target == i for target in viz_original['target']]
                sepal_vals = [viz_original['sepal_length'][j] for j in range(len(mask)) if mask[j]]
                petal_vals = [viz_original['petal_length'][j] for j in range(len(mask)) if mask[j]]
                
                ax1.scatter(sepal_vals, petal_vals, 
                          c=colores[i], label=especie.capitalize(), 
                          alpha=0.6, edgecolors='black', s=80)
            
            ax1.set_xlabel('Sepal Length (cm)', fontsize=12)
            ax1.set_ylabel('Petal Length (cm)', fontsize=12)
            ax1.set_title('Distribuci√≥n Original por Clase', fontsize=14, fontweight='bold')
            ax1.legend(title='Especies')
            ax1.grid(True, alpha=0.3)
            
            st.pyplot(fig1)
        
        with col2:
            st.markdown("**üìä Datos Estandarizados**")
            fig2, ax2 = plt.subplots(figsize=(8, 6))
            
            viz_estandarizado = resumen['visualizacion']['estandarizado']
            
            for i, especie in enumerate(nombres_especies):
                mask = [target == i for target in viz_estandarizado['target']]
                sepal_vals = [viz_estandarizado['sepal_length'][j] for j in range(len(mask)) if mask[j]]
                petal_vals = [viz_estandarizado['petal_length'][j] for j in range(len(mask)) if mask[j]]
                
                ax2.scatter(sepal_vals, petal_vals, 
                          c=colores[i], label=especie.capitalize(), 
                          alpha=0.6, edgecolors='black', s=80)
            
            ax2.set_xlabel('Sepal Length (Estandarizada)', fontsize=12)
            ax2.set_ylabel('Petal Length (Estandarizada)', fontsize=12)
            ax2.set_title('Distribuci√≥n Estandarizada por Clase', fontsize=14, fontweight='bold')
            ax2.legend(title='Especies')
            ax2.grid(True, alpha=0.3)
            
            st.pyplot(fig2)
        
        # 6. ESTAD√çSTICAS DESCRIPTIVAS
        st.markdown("---")
        st.markdown("## üìä Estad√≠sticas Descriptivas del Dataset Estandarizado")
        
        st.dataframe(resumen['estadisticas']['dataframe'], use_container_width=True)
        
        # Visualizaciones adicionales
        st.markdown("### üìà Visualizaciones Adicionales")
        
        # Gr√°fico de todas las caracter√≠sticas
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('Distribuci√≥n de Todas las Caracter√≠sticas por Clase', fontsize=16, fontweight='bold')
        
        feature_names = resumen['division']['caracteristicas']
        
        for idx, feature in enumerate(feature_names):
            row = idx // 2
            col = idx % 2
            
            for i, especie in enumerate(nombres_especies):
                mask = resumen['df_procesado']['target'] == i
                data = resumen['df_procesado'][mask][feature]
                axes[row, col].hist(data, bins=15, alpha=0.5, label=especie.capitalize(), 
                                   color=colores[i], edgecolor='black')
            
            feature_label = feature.replace(' (cm)', '').title()
            axes[row, col].set_xlabel(feature_label, fontsize=10)
            axes[row, col].set_ylabel('Frecuencia', fontsize=10)
            axes[row, col].set_title(f'Distribuci√≥n de {feature_label}', fontsize=12)
            axes[row, col].legend()
            axes[row, col].grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
        
        # Matriz de correlaci√≥n
        st.markdown("### üî• Matriz de Correlaci√≥n de Caracter√≠sticas")
        
        fig_corr, ax_corr = plt.subplots(figsize=(10, 8))
        
        feature_cols = [col for col in resumen['df_procesado'].columns 
                       if col not in ['target', 'species']]
        corr_matrix = resumen['df_procesado'][feature_cols].corr()
        
        # Nombres simplificados para la matriz
        labels_cortos = [col.replace(' (cm)', '').replace('sepal', 'Sep').replace('petal', 'Pet') 
                        for col in feature_cols]
        
        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
                   center=0, square=True, linewidths=1,
                   xticklabels=labels_cortos, yticklabels=labels_cortos,
                   cbar_kws={"shrink": 0.8}, ax=ax_corr)
        ax_corr.set_title('Matriz de Correlaci√≥n - Dataset Iris', fontsize=14, fontweight='bold')
        
        st.pyplot(fig_corr)
        
        # Descarga
        st.markdown("---")
        st.markdown("### üíæ Descargar Datos Procesados")
        
        csv = resumen['df_procesado'].to_csv(index=False)
        st.download_button(
            label="üì• Descargar CSV Procesado",
            data=csv,
            file_name="iris_procesado.csv",
            mime="text/csv",
            use_container_width=True
        )
        
    else:
        st.info("üëÜ Haz clic en el bot√≥n **'Ejecutar Procesamiento Completo'** para comenzar el an√°lisis.")
